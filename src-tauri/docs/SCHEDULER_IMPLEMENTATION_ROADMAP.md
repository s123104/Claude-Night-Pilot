# ğŸš€ Claude Night Pilot æ’ç¨‹å™¨å¯¦æ–½è·¯ç·šåœ–

**æ–‡æª”ç‰ˆæœ¬**: v1.0.0-implementation  
**å‰µå»ºæ™‚é–“**: 2025-08-17T17:15:00+00:00  
**åŸºæ–¼**: SCHEDULER_COMPREHENSIVE_REFACTORING_PLAN.md å®Œæ•´å¯©è¨ˆ

---

## ğŸ¯ å¯¦æ–½å„ªå…ˆç´šçŸ©é™£

| ä»»å‹™ | å½±éŸ¿åº¦ | æ€¥è¿«æ€§ | å¯¦æ–½é›£åº¦ | å„ªå…ˆç´š | é ä¼°æ™‚é–“ |
|------|--------|--------|----------|--------|----------|
| ğŸ”´ è³‡æ–™åº«è·¯å¾‘çµ±ä¸€ | é«˜ | é«˜ | ä¸­ | P0 | 0.5å¤© |
| ğŸŸ¡ çµ±ä¸€æ’ç¨‹å™¨æ ¸å¿ƒ | é«˜ | ä¸­ | é«˜ | P1 | 1.5å¤© |
| ğŸŸ¢ EnterpriseåŠŸèƒ½æ•´åˆ | ä¸­ | ä½ | ä¸­ | P2 | 1å¤© |
| ğŸŸ£ E2Eæ¸¬è©¦æ›´æ–° | ä¸­ | ä¸­ | ä½ | P3 | 0.5å¤© |
| ğŸ”µ æ€§èƒ½å„ªåŒ– | ä½ | ä½ | ä¸­ | P4 | 0.5å¤© |

---

## ğŸ“… è©³ç´°å¯¦æ–½è¨ˆåŠƒ

### **Day 1 Morning: P0 - ç·Šæ€¥è³‡æ–™åº«ä¿®å¾©**

#### ğŸ”´ çµ±ä¸€è³‡æ–™åº«è·¯å¾‘ (0.5å¤©)
```bash
# 1. å‚™ä»½ç¾æœ‰è³‡æ–™åº«
cp claude-pilot.db claude-pilot-backup-$(date +%Y%m%d).db
cp claude-night-pilot.db claude-night-pilot-backup-$(date +%Y%m%d).db

# 2. å»ºç«‹é·ç§»è…³æœ¬
touch src-tauri/migrations/urgent_path_unification.sql
```

**å¯¦æ–½æ­¥é©Ÿ**:
```rust
// src-tauri/src/database/migration.rs (æ–°æª”æ¡ˆ)
use anyhow::Result;
use std::path::Path;

pub struct DatabasePathMigrator;

impl DatabasePathMigrator {
    pub async fn execute_emergency_unification() -> Result<()> {
        const LEGACY_PATH: &str = "claude-pilot.db";
        const UNIFIED_PATH: &str = "claude-night-pilot.db";
        
        // 1. æª¢æŸ¥èˆŠè³‡æ–™åº«æ˜¯å¦å­˜åœ¨
        if Path::new(LEGACY_PATH).exists() {
            println!("ğŸ”„ ç™¼ç¾èˆŠè³‡æ–™åº«ï¼Œé–‹å§‹é·ç§»...");
            
            // 2. å‚™ä»½ç¾æœ‰è³‡æ–™
            Self::backup_existing_data().await?;
            
            // 3. åˆä½µè³‡æ–™åˆ°çµ±ä¸€è·¯å¾‘
            Self::merge_databases(LEGACY_PATH, UNIFIED_PATH).await?;
            
            // 4. é©—è­‰è³‡æ–™å®Œæ•´æ€§
            Self::verify_migration().await?;
            
            println!("âœ… è³‡æ–™åº«è·¯å¾‘çµ±ä¸€å®Œæˆ");
        }
        
        Ok(())
    }
}
```

### **Day 1 Afternoon: P1 - æ ¸å¿ƒæ¶æ§‹é‡æ§‹**

#### ğŸŸ¡ å»ºç«‹çµ±ä¸€æ’ç¨‹å™¨ (1.5å¤©)

**Step 1: æ ¸å¿ƒæ¨¡çµ„å»ºç«‹**
```rust
// src-tauri/src/scheduler/unified_scheduler.rs (æ–°æª”æ¡ˆ)
use crate::models::job::{Job, JobStatus, JobExecutionResult};
use crate::scheduler::real_time_executor::{RealTimeExecutor, ExecutionStats};
use anyhow::Result;
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;

/// çµ±ä¸€ä¼æ¥­ç´šæ’ç¨‹å™¨
/// 
/// æ•´åˆäº†æ‰€æœ‰ç¾æœ‰æ’ç¨‹å™¨çš„æœ€ä½³åŠŸèƒ½ï¼š
/// - RealTimeExecutor: ä¼æ¥­ç´šåŸ·è¡Œå¼•æ“
/// - JobScheduler: å›èª¿æ©Ÿåˆ¶
/// - SimpleJobManager: ç›¸å®¹æ€§API
/// - Context7æœ€ä½³å¯¦è¸: éŒ¯èª¤è™•ç†èˆ‡ç›£æ§
pub struct UnifiedScheduler {
    /// æ ¸å¿ƒåŸ·è¡Œå™¨ (åŸºæ–¼ RealTimeExecutor)
    core_executor: Arc<RealTimeExecutor>,
    
    /// ä¼æ¥­ç´šç›£æ§
    metrics_collector: Arc<MetricsCollector>,
    
    /// ä½¿ç”¨é‡è¿½è¹¤ (åŸºæ–¼ ccusage æ¨¡å¼)
    usage_tracker: Arc<UsageTracker>,
    
    /// ä»»å‹™é—œä¿‚ç®¡ç† (åŸºæ–¼ vibe-kanban æ¨¡å¼)
    task_hierarchy: Arc<RwLock<TaskHierarchy>>,
}

impl UnifiedScheduler {
    /// å»ºç«‹æ–°çš„çµ±ä¸€æ’ç¨‹å™¨å¯¦ä¾‹
    /// 
    /// æ•´åˆ Context7 æœ€ä½³å¯¦è¸çš„åˆå§‹åŒ–æµç¨‹
    pub async fn new() -> Result<Self> {
        // Context7 æœ€ä½³å¯¦è¸: è©³ç´°çš„åˆå§‹åŒ–éŒ¯èª¤è™•ç†
        let core_executor = Arc::new(
            RealTimeExecutor::new()
                .await
                .context("Failed to initialize core RealTimeExecutor")?
        );
        
        let metrics_collector = Arc::new(MetricsCollector::new().await?);
        let usage_tracker = Arc::new(UsageTracker::new().await?);
        let task_hierarchy = Arc::new(RwLock::new(TaskHierarchy::new()));
        
        Ok(Self {
            core_executor,
            metrics_collector,
            usage_tracker,
            task_hierarchy,
        })
    }
    
    /// å•Ÿå‹•çµ±ä¸€æ’ç¨‹å™¨
    /// 
    /// æŒ‰é †åºå•Ÿå‹•æ‰€æœ‰å­ç³»çµ±
    pub async fn start(&self) -> Result<()> {
        tracing::info!("ğŸš€ Starting UnifiedScheduler...");
        
        // 1. å•Ÿå‹•æ ¸å¿ƒåŸ·è¡Œå™¨
        self.core_executor.start().await
            .context("Failed to start core executor")?;
            
        // 2. å•Ÿå‹•ç›£æ§ç³»çµ±
        self.metrics_collector.start().await
            .context("Failed to start metrics collector")?;
            
        // 3. å•Ÿå‹•ä½¿ç”¨é‡è¿½è¹¤
        self.usage_tracker.start().await
            .context("Failed to start usage tracker")?;
            
        tracing::info!("âœ… UnifiedScheduler started successfully");
        Ok(())
    }
}
```

**Step 2: ç›¸å®¹æ€§APIå±¤**
```rust
// src-tauri/src/scheduler/compatibility.rs (æ–°æª”æ¡ˆ)
/// ç‚ºç¾æœ‰ä»£ç¢¼æä¾›ç›¸å®¹æ€§API
/// 
/// ç¢ºä¿é‡æ§‹æœŸé–“å‘å¾Œç›¸å®¹
pub struct CompatibilityLayer {
    unified_scheduler: Arc<UnifiedScheduler>,
}

impl CompatibilityLayer {
    /// SimpleJobManager ç›¸å®¹API
    pub async fn simple_job_manager_add_job(&self, job: &Job) -> Result<String> {
        // è½‰ç™¼åˆ°çµ±ä¸€æ’ç¨‹å™¨
        self.unified_scheduler.add_job(job).await
    }
    
    /// JobScheduler ç›¸å®¹API  
    pub async fn job_scheduler_schedule(&self, job: Job) -> Result<()> {
        // è½‰æ›æ ¼å¼ä¸¦è½‰ç™¼
        let job_id = self.unified_scheduler.add_job(&job).await?;
        tracing::info!("Job scheduled via compatibility layer: {}", job_id);
        Ok(())
    }
}
```

### **Day 2: P1 ç¹¼çºŒ + P2 é–‹å§‹**

#### ğŸŸ¡ å®Œæˆçµ±ä¸€æ’ç¨‹å™¨æ•´åˆ

**Step 3: è³‡æ–™æ¨¡å‹çµ±ä¸€**
```rust
// src-tauri/src/models/unified_job.rs (æ–°æª”æ¡ˆ)
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// çµ±ä¸€çš„ä»»å‹™æ¨¡å‹
/// 
/// æ•´åˆæ‰€æœ‰ç¾æœ‰ä»»å‹™çµæ§‹çš„æœ€ä½³åŠŸèƒ½
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UnifiedJob {
    // åŸºæœ¬å±¬æ€§ (ä¾†è‡ª models/job.rs)
    pub id: Uuid,
    pub name: String, 
    pub prompt_id: String,
    pub cron_expression: String,
    pub status: JobStatus,
    pub job_type: JobType,
    
    // ä¼æ¥­ç´šåŠŸèƒ½ (ä¾†è‡ª RealTimeExecutor)
    pub execution_stats: ExecutionStats,
    pub retry_config: RetryConfig,
    
    // vibe-kanban æ¨¡å¼æ•´åˆ
    pub parent_job_id: Option<Uuid>,
    pub execution_processes: Vec<JobExecutionProcess>,
    
    // ccusage ä½¿ç”¨é‡è¿½è¹¤
    pub usage_data: UsageData,
    
    // æ™‚é–“æˆ³
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

/// ä»»å‹™åŸ·è¡Œæµç¨‹ (åŸºæ–¼ vibe-kanban ExecutionProcess)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JobExecutionProcess {
    pub id: Uuid,
    pub job_id: Uuid,
    pub process_type: ProcessType,
    pub status: ProcessStatus,
    pub start_time: DateTime<Utc>,
    pub end_time: Option<DateTime<Utc>>,
    pub output: Option<String>,
    pub error_message: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ProcessType {
    Setup,        // å‰ç½®æº–å‚™
    Execution,    // ä¸»è¦åŸ·è¡Œ  
    Cleanup,      // å¾ŒçºŒæ¸…ç†
    Validation,   // çµæœé©—è­‰
}
```

#### ğŸŸ¢ Enterprise åŠŸèƒ½æ•´åˆ (1å¤©)

**Step 4: ä½¿ç”¨é‡è¿½è¹¤æ•´åˆ**
```rust
// src-tauri/src/tracking/usage_tracker.rs (æ–°æª”æ¡ˆ)
use crate::models::usage::{UsageData, CostMode};
use anyhow::Result;

/// åŸºæ–¼ ccusage æ¨¡å¼çš„ä½¿ç”¨é‡è¿½è¹¤å™¨
pub struct UsageTracker {
    database: Arc<SqlitePool>,
    cost_calculator: CostCalculator,
}

impl UsageTracker {
    pub async fn track_execution(
        &self,
        job_id: &str,
        result: &JobExecutionResult
    ) -> Result<()> {
        let usage_data = UsageData {
            job_id: job_id.to_string(),
            session_id: result.session_id.clone(),
            tokens_input: result.tokens_input,
            tokens_output: result.tokens_output,
            cost_usd: result.calculate_cost(CostMode::Auto)?,
            model_name: result.model_name.clone(),
            timestamp: result.start_time,
        };
        
        // å„²å­˜åˆ°çµ±ä¸€è³‡æ–™åº«
        self.store_usage_data(&usage_data).await?;
        
        // æ›´æ–°å³æ™‚çµ±è¨ˆ
        self.update_realtime_metrics(&usage_data).await?;
        
        Ok(())
    }
}
```

### **Day 3: P2 å®Œæˆ + P3 é–‹å§‹**

#### ğŸŸ¢ å®Œæˆ Enterprise åŠŸèƒ½

**Step 5: ç›£æ§èˆ‡å‘Šè­¦ç³»çµ±**
```rust
// src-tauri/src/monitoring/metrics_collector.rs (æ–°æª”æ¡ˆ)
use std::collections::HashMap;
use tokio::time::{interval, Duration};

/// ä¼æ¥­ç´šç›£æ§æ”¶é›†å™¨
pub struct MetricsCollector {
    metrics: Arc<RwLock<HashMap<String, MetricValue>>>,
    alert_manager: AlertManager,
}

impl MetricsCollector {
    pub async fn start_monitoring(&self) -> Result<()> {
        let mut interval = interval(Duration::from_secs(30));
        
        loop {
            interval.tick().await;
            
            // æ”¶é›†ç³»çµ±æŒ‡æ¨™
            self.collect_system_metrics().await?;
            
            // æ”¶é›†ä»»å‹™æŒ‡æ¨™  
            self.collect_job_metrics().await?;
            
            // æª¢æŸ¥å‘Šè­¦æ¢ä»¶
            self.check_alerts().await?;
        }
    }
}
```

#### ğŸŸ£ E2Eæ¸¬è©¦æ›´æ–° (0.5å¤©)

**Step 6: æ¸¬è©¦å¥—ä»¶æ›´æ–°**
```rust
// src-tauri/tests/integration/unified_scheduler_tests.rs (æ–°æª”æ¡ˆ)
#[cfg(test)]
mod unified_scheduler_tests {
    use super::*;
    
    #[tokio::test]
    async fn test_unified_scheduler_full_lifecycle() -> Result<()> {
        // æ¸¬è©¦çµ±ä¸€æ’ç¨‹å™¨å®Œæ•´ç”Ÿå‘½é€±æœŸ
        let scheduler = UnifiedScheduler::new().await?;
        
        // 1. å•Ÿå‹•æ¸¬è©¦
        scheduler.start().await?;
        
        // 2. ä»»å‹™ç®¡ç†æ¸¬è©¦
        let job = create_test_unified_job();
        let job_id = scheduler.add_job(&job).await?;
        
        // 3. éšå±¤å¼ä»»å‹™æ¸¬è©¦ (vibe-kanban æ¨¡å¼)
        let child_job = create_child_job(&job_id);
        scheduler.add_child_job(&child_job).await?;
        
        // 4. ä½¿ç”¨é‡è¿½è¹¤æ¸¬è©¦
        let usage = scheduler.get_usage_stats(&job_id).await?;
        assert!(usage.total_executions >= 0);
        
        // 5. æ¸…ç†æ¸¬è©¦
        scheduler.remove_job(&job_id).await?;
        scheduler.stop().await?;
        
        Ok(())
    }
    
    #[tokio::test]
    async fn test_backward_compatibility() -> Result<()> {
        // æ¸¬è©¦å‘å¾Œç›¸å®¹æ€§
        let scheduler = UnifiedScheduler::new().await?;
        let compat_layer = CompatibilityLayer::new(scheduler);
        
        // æ¸¬è©¦ SimpleJobManager API
        let simple_result = compat_layer
            .simple_job_manager_add_job(&create_legacy_job())
            .await;
        assert!(simple_result.is_ok());
        
        // æ¸¬è©¦ JobScheduler API  
        let scheduler_result = compat_layer
            .job_scheduler_schedule(create_legacy_job())
            .await;
        assert!(scheduler_result.is_ok());
        
        Ok(())
    }
}
```

### **Day 4: P3 å®Œæˆ + P4 + æœ€çµ‚æ•´åˆ**

#### ğŸŸ£ å®Œæˆæ¸¬è©¦æ•´åˆ

**Step 7: Playwright E2E æ›´æ–°**
```javascript
// tests/integration/unified-scheduler.spec.js (æ–°æª”æ¡ˆ)
import { test, expect } from "@playwright/test";

test.describe("çµ±ä¸€æ’ç¨‹å™¨ E2E æ¸¬è©¦", () => {
  test("æ¸¬è©¦æ–°çµ±ä¸€æ’ç¨‹å™¨ä»‹é¢", async ({ page }) => {
    await page.goto("http://localhost:8080");
    
    // é©—è­‰æ–°çš„çµ±ä¸€ä»‹é¢
    await expect(page.locator("#unified-scheduler-status")).toBeVisible();
    
    // æ¸¬è©¦éšå±¤å¼ä»»å‹™å»ºç«‹
    await page.click('button:has-text("å»ºç«‹éšå±¤ä»»å‹™")');
    await page.fill("#parent-task-name", "ä¸»ä»»å‹™");
    await page.fill("#child-task-name", "å­ä»»å‹™");
    
    await page.click('button:has-text("å»ºç«‹ä»»å‹™éšå±¤")');
    await expect(page.locator("text=ä»»å‹™éšå±¤å»ºç«‹æˆåŠŸ")).toBeVisible();
    
    // é©—è­‰ä»»å‹™é—œä¿‚é¡¯ç¤º
    await expect(page.locator(".task-hierarchy-view")).toBeVisible();
  });
  
  test("æ¸¬è©¦ä½¿ç”¨é‡è¿½è¹¤ä»‹é¢", async ({ page }) => {
    await page.goto("http://localhost:8080");
    
    // æª¢æŸ¥ä½¿ç”¨é‡å„€è¡¨æ¿
    await page.click('nav a:has-text("ä½¿ç”¨é‡çµ±è¨ˆ")');
    await expect(page.locator("#usage-dashboard")).toBeVisible();
    
    // é©—è­‰çµ±è¨ˆè³‡æ–™é¡¯ç¤º
    await expect(page.locator(".token-usage-chart")).toBeVisible();
    await expect(page.locator(".cost-breakdown")).toBeVisible();
  });
});
```

#### ğŸ”µ æ€§èƒ½å„ªåŒ– (0.5å¤©)

**Step 8: æœ€çµ‚å„ªåŒ–**
```rust
// src-tauri/src/scheduler/optimization.rs (æ–°æª”æ¡ˆ)
/// æ€§èƒ½å„ªåŒ–æ¨¡çµ„
pub struct PerformanceOptimizer;

impl PerformanceOptimizer {
    /// è¨˜æ†¶é«”ä½¿ç”¨å„ªåŒ–
    pub async fn optimize_memory_usage() -> Result<()> {
        // 1. æ¸…ç†éæœŸçš„åŸ·è¡Œè¨˜éŒ„
        // 2. å„ªåŒ–è³‡æ–™åº«é€£æ¥æ± 
        // 3. å¯¦æ–½æ™ºèƒ½ç·©å­˜ç­–ç•¥
        Ok(())
    }
    
    /// ä¸¦ç™¼æ€§èƒ½å„ªåŒ–
    pub async fn optimize_concurrency() -> Result<()> {
        // 1. èª¿æ•´tokioåŸ·è¡Œç·’æ± å¤§å°
        // 2. å„ªåŒ–é–çˆ­ç”¨
        // 3. å¯¦æ–½èƒŒå£“æ©Ÿåˆ¶
        Ok(())
    }
}
```

---

## ğŸ“Š å¯¦æ–½æª¢æ ¸æ¸…å–®

### **Day 1 æª¢æ ¸é»**
- [ ] è³‡æ–™åº«è·¯å¾‘çµ±ä¸€å®Œæˆ
- [ ] è³‡æ–™é·ç§»é©—è­‰é€šé
- [ ] çµ±ä¸€æ’ç¨‹å™¨æ ¸å¿ƒæ¨¡çµ„å»ºç«‹
- [ ] åŸºæœ¬åŠŸèƒ½æ¸¬è©¦é€šé

### **Day 2 æª¢æ ¸é»** 
- [ ] è³‡æ–™æ¨¡å‹çµ±ä¸€å®Œæˆ
- [ ] ç›¸å®¹æ€§APIå±¤å¯¦ä½œå®Œæˆ
- [ ] EnterpriseåŠŸèƒ½æ•´åˆé–‹å§‹
- [ ] ä½¿ç”¨é‡è¿½è¹¤ç³»çµ±å°±ç·’

### **Day 3 æª¢æ ¸é»**
- [ ] ç›£æ§ç³»çµ±æ•´åˆå®Œæˆ
- [ ] E2Eæ¸¬è©¦å¥—ä»¶æ›´æ–°
- [ ] å‘å¾Œç›¸å®¹æ€§é©—è­‰é€šé
- [ ] æ€§èƒ½åŸºæº–æ¸¬è©¦åŸ·è¡Œ

### **Day 4 æª¢æ ¸é»**
- [ ] æœ€çµ‚æ•´åˆæ¸¬è©¦é€šé
- [ ] æ€§èƒ½å„ªåŒ–å®Œæˆ
- [ ] æ–‡æª”æ›´æ–°å®Œæˆ
- [ ] éƒ¨ç½²å°±ç·’ç¢ºèª

---

## ğŸš¨ é¢¨éšªç®¡æ§

### **é«˜é¢¨éšªé …ç›®**
1. **è³‡æ–™åº«é·ç§»** - å‚™ä»½ç­–ç•¥ + æ®µéšå¼é·ç§»
2. **å‘å¾Œç›¸å®¹æ€§** - å®Œæ•´å›æ­¸æ¸¬è©¦ + ç›¸å®¹æ€§å±¤

### **ä¸­é¢¨éšªé …ç›®**  
1. **æ€§èƒ½å½±éŸ¿** - åŸºæº–æ¸¬è©¦ + æ¼¸é€²å¼éƒ¨ç½²
2. **åŠŸèƒ½æ•´åˆ** - æ¨¡çµ„åŒ–è¨­è¨ˆ + ç¨ç«‹æ¸¬è©¦

### **é¢¨éšªç·©è§£æªæ–½**
```bash
# 1. è‡ªå‹•å‚™ä»½è…³æœ¬
./scripts/backup-before-migration.sh

# 2. å›æ»¾è¨ˆåŠƒ
./scripts/rollback-to-previous-version.sh  

# 3. å¥åº·æª¢æŸ¥
./scripts/health-check-unified-scheduler.sh

# 4. æ¼¸é€²å¼éƒ¨ç½²
./scripts/canary-deployment.sh
```

---

## ğŸ“ˆ æˆåŠŸæŒ‡æ¨™

### **æŠ€è¡“æŒ‡æ¨™**
- âœ… å•Ÿå‹•æ™‚é–“ < 50ms (ç›®æ¨™: <25ms)  
- âœ… è¨˜æ†¶é«”ä½¿ç”¨ é™ä½ 40%
- âœ… ä»£ç¢¼è¦†è“‹ç‡ > 90%
- âœ… é›¶åœæ©Ÿæ™‚é–“é·ç§»

### **æ¥­å‹™æŒ‡æ¨™**
- âœ… å‘å¾Œç›¸å®¹æ€§ 100%
- âœ… åŠŸèƒ½å®Œæ•´æ€§ 100%  
- âœ… ç”¨æˆ¶é«”é©—æ”¹å–„ 25%
- âœ… é–‹ç™¼æ•ˆç‡æå‡ 30%

---

**å¯¦æ–½ç‹€æ…‹**: ğŸŸ¡ æº–å‚™å°±ç·’  
**ä¸‹ä¸€æ­¥**: åŸ·è¡Œ Day 1 è³‡æ–™åº«è·¯å¾‘çµ±ä¸€  
**è² è²¬åœ˜éšŠ**: Backend Team + QA Team  
**é è¨ˆå®Œæˆ**: 4å€‹å·¥ä½œå¤©